---
layout:     post
title:      bertå¾®è°ƒ
subtitle:   é’ˆå¯¹æ–‡æœ¬åˆ†ç±»ä»»åŠ¡çš„bertå¾®è°ƒ
date:       2023-11-23
author:     Klaus
header-img: img/post-bg-cook.jpg
catalog: true
tags:
    - bert
    - fine-tune
    - æ–‡æœ¬åˆ†ç±»
---

## å‰è¨€

- è¿™æ˜¯å¸®æˆ‘å°å¯¼è®­ç»ƒçš„`æ–‡æœ¬åˆ†ç±»`æ¨¡å‹ï¼Œç”±äºä¹‹å‰éƒ½æ˜¯çº¸ä¸Šè°ˆå…µæ‰€ä»¥èµ°äº†å¾ˆå¤šå¼¯è·¯ã€‚
- æ¨¡å‹æ–¹é¢æœ€å¼€å§‹ç”¨è¿‡mlpå’Œlstmã€‚embeddingæ–¹å¼è‡ªå·±å°è¯•è¿‡one-hotã€word2vecã€è¯é¢‘ç»Ÿè®¡ç­‰ã€‚å‡†ç¡®ç‡éƒ½åªæœ‰60%å·¦å³ã€‚
- ä½†æ˜¯æœ€ç»ˆå‘ç°`bert-base-chinese`è¿™ä¸ªå¥½ä¸œè¥¿ï¼Œå‡†ç¡®ç‡90%å¤šï¼ˆå¯èƒ½æœ‰ç‚¹è¿‡æ‹Ÿåˆï¼‰ã€‚
- **è¿˜è¦å†™çš„è§„èŒƒä¸€ç‚¹ï¼Œå†™æˆåƒè®ºæ–‡ä»£ç é‚£ç§ã€‚**

## æ•°æ®é›†ä»‹ç»

- å¾®åšä¸Šçˆ¬å–çš„ç»¼è‰ºèŠ‚ç›®è¯„è®ºã€‚èŠ‚ç›®åŒ…æ‹¬ã€Šåªæ­¤é’ç»¿ã€‹ã€ã€Šä¸Šæ–°äº†Â·æ•…å®«ã€‹ã€ã€Šå¦‚æœå›½å®ä¼šè¯´è¯ã€‹ã€ã€Šæˆ‘åœ¨æ•…å®«å…­ç™¾å¹´ã€‹ã€ã€Šå›½å®¶å®è—ã€‹ç­‰ã€‚å·²äººå·¥æ ‡æ³¨ç±»åˆ«çš„æœ‰8665æ¡ï¼Œå…¶ä»–10000å¤šæ¡æœªæ ‡æ³¨ã€‚
- ä»¥ä¸‹æ˜¯åˆ†ç±»æ ‡å‡†ï¼š

	|	Label	|	Description	|
	|	:------:	|	:----	|
	|	0	|	æåˆ°èŠ‚ç›®åˆ¶ä½œã€æ¼”å‘˜ã€èŠ‚ç›®å¥½çœ‹ç¨‹åº¦|
	|	1	|	æåˆ°èŠ‚ç›®ä¸­çš„æ–‡ç‰©ã€å¤ç±ã€äººç‰©|
	|	2	|	æåˆ°èŠ‚ç›®æ‰€ä¼ è¾¾çš„æ–‡åŒ–ã€è‰ºæœ¯ã€ç²¾ç¥ã€å›½å®¶|
	|	3	|	æ— å…³è¯„è®º	|

## å®éªŒè¿‡ç¨‹ä»£ç 

### 1. `GPU`åŠ`requirements.txt`
- GPU: 4 \* RTX 6000 (24G)  (å…¶å®ä¸€å—å°±å¤Ÿï¼‰
- requirements:
```python
pandas==2.1.4
scikit-learn==1.3.0
numpy==1.26.4
matplotlib==3.8.0
torch==2.2.2
```

### 2. import & æ–‡ä»¶å¯¼å…¥(dataset + model)
- import 
```python
import pandas as pd
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, TensorDataset
from transformers import BertTokenizer, BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import precision_score, recall_score, f1_score
```

- å¯¼å…¥modelï¼ˆç”±äºå­¦é™¢æœåŠ¡å™¨ä¸èƒ½æ¥vpnï¼Œéœ€è¦ä»huggingfaceä¸‹è½½å¥½modelå†ä¸Šä¼ åˆ°æœåŠ¡å™¨ï¼‰
```python
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
model_path = '/path/to/hugging_face/bert-base-chinese-download-part'
token = BertTokenizer.from_pretrained(model_path,cache_dir=None,force_download=False)
# åŠ è½½é¢„è®­ç»ƒçš„BERTæ¨¡å‹

model = BertForSequenceClassification.from_pretrained(model_path,num_labels=4).to(device)
# å¦‚æœæœ‰å¤šä¸ªGPUï¼Œä½¿ç”¨DataParallelåŒ…è£…æ¨¡å‹

if torch.cuda.device_count() > 1:
	print("Using", torch.cuda.device_count(), "GPUs!")
	model = nn.DataParallel(model)
# model.load_state_dict(torch.load('bert_model.pth')) # å¦‚æœä¸è¦æœ¬åœ°çš„æ¨¡å‹ï¼Œå°±å¿½ç•¥
```
	
> model
 
```python
DataParallel(
	 (module): BertForSequenceClassification(
	   (bert): BertModel(
	     (embeddings): BertEmbeddings(
		(word_embeddings): Embedding(21128, 768, padding_idx=0)
		(position_embeddings): Embedding(512, 768)
		(token_type_embeddings): Embedding(2, 768)
		(LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
		(dropout): Dropout(p=0.1, inplace=False)
	      )
	      (encoder): BertEncoder(
		(layer): ModuleList(
		  (0-11): 12 x BertLayer(
		    (attention): BertAttention(
		      (self): BertSelfAttention(
			(query): Linear(in_features=768, out_features=768, bias=True)
			(key): Linear(in_features=768, out_features=768, bias=True)
			(value): Linear(in_features=768, out_features=768, bias=True)
			(dropout): Dropout(p=0.1, inplace=False)
		      )
		      (output): BertSelfOutput(
			(dense): Linear(in_features=768, out_features=768, bias=True)
			(LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
			(dropout): Dropout(p=0.1, inplace=False)
		      )
		    )
		    (intermediate): BertIntermediate(
		      (dense): Linear(in_features=768, out_features=3072, bias=True)
		      (intermediate_act_fn): GELUActivation()
		    )
		    (output): BertOutput(
		      (dense): Linear(in_features=3072, out_features=768, bias=True)
		      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
		      (dropout): Dropout(p=0.1, inplace=False)
		    )
		  )
		)
	      )
	      (pooler): BertPooler(
		(dense): Linear(in_features=768, out_features=768, bias=True)
		(activation): Tanh()
	      )
	    )
	    (dropout): Dropout(p=0.1, inplace=False)
	    (classifier): Linear(in_features=768, out_features=4, bias=True)
	  )
	 )
```

- å¯¼å…¥dataset
```python
df = pd.read_excel('æ€»8665.xlsx')
sentence = np.array(df['Sentence'].tolist())
label = np.array(df['Label'].tolist())
```

> df[4000:4003] 		# æ•°æ®é›†å±•ç¤º

```
(        Label                                         Sentence
 4000      0                                 æ”¶ä¸€å¼ å¹¿å·1015æ™š680ä»¥ä¸‹ 
 4001      0   2023ç¬¬ä¸€ä¸ªæœˆçš„æ‰‹å¸è®°å½•!ã€Šâ€œåªæ­¤é’ç»¿â€ã€‹è¯¶å‘€è¿‡å¹´çœŸçš„å¾ˆå®¹æ˜“å¿˜è®°å†™æ‰‹å¸è¯¶,ç©ºçª—äº†ä¸å°‘æ—¥å­ 
 4002      0                          å§å¦¹ä»¬å¤§è¿åœºä¼šæœ‰sdå—æˆ‘çœŸçš„å¤ªæƒ³è§åˆ°å­Ÿåº†æ—¸äº† )
```
 
### 3. è®­ç»ƒè¿‡ç¨‹

```python
kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

for fold, (train_idx, test_idx) in enumerate(kf.split(sentence, label)):
    print(f"Fold {fold + 1}")
    # åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†

    X_train, X_test = sentence[train_idx], sentence[test_idx]
    y_train, y_test = label[train_idx], label[test_idx]

    # æ•°æ®å¤„ç†å’Œå‡†å¤‡

    train_inputs = [token.encode(sen, add_special_tokens=True, max_length=100, truncation=True, return_tensors='pt', padding='max_length').view(-1) for sen in X_train]
    test_inputs = [token.encode(sen, add_special_tokens=True, max_length=100, truncation=True, return_tensors='pt', padding='max_length').view(-1) for sen in X_test]

    train_labels = torch.tensor(y_train)
    test_labels = torch.tensor(y_test)

    dataset_train = TensorDataset(torch.stack(train_inputs), train_labels)
    dataloader_train = DataLoader(dataset_train, batch_size=512, shuffle=True)

    dataset_test = TensorDataset(torch.stack(test_inputs), test_labels)
    dataloader_test = DataLoader(dataset_test, batch_size=256, shuffle=False)
    
    # å®šä¹‰æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨

    criterion = nn.CrossEntropyLoss()
    optimizer = AdamW(model.parameters(), lr=1e-5)
    num_epochs = 5
    total_steps = len(dataloader_train) * num_epochs
    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)

    for epoch in range(num_epochs):
	model.train()
	total_loss = 0.0
	for inputs, labels in dataloader_train:
	    inputs, labels = inputs.to(device), labels.to(device)
	    optimizer.zero_grad()
	    outputs = model(inputs)[0]
	    labels = labels.long()
	    loss = criterion(outputs, labels)
	    loss.backward()
	    optimizer.step()
	    scheduler.step()

	    total_loss += loss.item()

	avg_loss = total_loss / len(dataloader_train)
	print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {avg_loss}')

    # åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æ¨¡å‹

	if epoch % 5 == 4:
	    model.eval()
	    with torch.no_grad():
		correct = 0
		total = 0
		all_predicted = []
		all_labels = []
		for inputs, labels in dataloader_test:
		    inputs, labels = inputs.to(device), labels.to(device)
		    outputs = model(inputs)[0]
		    labels = labels.long()
		    _, predicted = torch.max(outputs, 1)
		    total += labels.size(0)
		    correct += (predicted == labels).sum().item()
		    
		    all_predicted.extend(predicted.cpu().numpy())
		    all_labels.extend(labels.cpu().numpy())

		accuracy = correct / total
		print(f'Test Accuracy: {accuracy}')
		
		# è®¡ç®—æ¯ä¸ªç±»åˆ«çš„ç²¾ç¡®åº¦ã€å¬å›ç‡ã€F1åˆ†æ•°

		precision = precision_score(all_labels, all_predicted, average=None)
		recall = recall_score(all_labels, all_predicted, average=None)
		f1 = f1_score(all_labels, all_predicted, average=None)

		for i in range(len(precision)):
		    print(f'Class {i} - Precision: {precision[i]}, Recall: {recall[i]}, F1 Score: {f1[i]}')

# ä¿å­˜æ¨¡å‹

torch.save(model.state_dict(), 'bert_model.pth')

# åŠ è½½æ¨¡å‹

model = BertForSequenceClassification.from_pretrained('bert-base-chinese', num_labels=4).to(device)
model.load_state_dict(torch.load('bert_model.pth'))

model.eval()  
# å¦‚æœåªæ˜¯ç”¨æ¥è¿›è¡Œæ¨ç†è€Œéç»§ç»­è®­ç»ƒï¼Œåˆ™éœ€è¦å°†æ¨¡å‹è®¾ç½®ä¸º evaluation æ¨¡å¼
```
## æ•ˆæœå±•ç¤º

é€‰æ‹©éƒ¨åˆ†æ ·æœ¬å¯¹å…¶è¿›è¡Œæ ‡ç­¾é¢„æµ‹

```python
# å‡è®¾sentencesæ˜¯å¾…é¢„æµ‹çš„å¥å­åˆ—è¡¨

sentences = ["çœ‹å…¸ç±é‡Œçš„ä¸­å›½è¦å¤‡å¥½çº¸æ“¦çœ¼æ³ª", 
	     "è¿™å‡ å¤©,è€³è¾¹çœ¼ä¸­å…¨æ˜¯å»ºå…š100å‘¨å¹´çš„äº‹,ä¹Ÿåˆšå¥½åœ¨çœ‹ã€Šè§‰é†’å¹´ä»£ã€‹,åŠ ä¹‹è¿‘æœŸçœ‹çš„å‰§ä¸æ˜¯å…³äºã€Šç†æƒ³ç…§è€€ä¸­å›½ã€‹,å°±æ˜¯å…³äºä¸­åæ–‡æ˜æºè¿œæµé•¿çš„ã€Šå…¸ç±é‡Œçš„ä¸­å›½ã€‹ã€‚è¶Šå‘è¶Šå‘åœ°åœ¨æ„Ÿæ…¨,è¿‘ä»£å²é‚£ä¸ªæ—¶æ®µçš„äºº,èƒ½åœ¨â€œå±±å¤ªå¤š,åº™å¤ªå°‘,ä¸çŸ¥é“é‚£åº§åº™èƒ½æ•‘ä¸­å›½â€çš„æ—¶å€™,å°±åšå®šç€å»è·µè¡Œ,å»å¥‰çŒ®,å»ç‰ºç‰²,æ˜¯ä½•ç­‰çš„æ¿€æ‰¬ä¸”çƒ­è¡€ã€‚ä¹Ÿè¶Šå‘æ„Ÿæ…¨äºä¸­åæ–‡æ˜ä¸€è·¯èµ°æ¥çš„ä¸æ˜“ä¸å£®é˜”,è½¦åŒè½¨,ä¹¦åŒæ–‡,å„ç§æ€æƒ³ä¸€ä»£ä»£ä¼ æ‰¿ä¸å‘æ‰¬,è¿‘ä»£æ¨å¹¿ç™½è¯æ–‡,æ€æƒ³å¼€åŒ–æ™®åŠ,æœ‰æ—¶å€™ä¼šè®¶å¼‚äº,å…ˆè´¤å‰è¾ˆä»¬æ€ä¹ˆå°±è¿™ä¹ˆå‰å®³,å„ç§é€‰æ‹©éƒ½èƒ½åˆ‡ä¸­æ—¶å¼Šã€‚æ­¤åˆ»åªè§‰å¾—,è‡ªå·±æ–‡ç¬”å¤ªå·®,å†™ä¸å‡ºå¿ƒä¸­çš„æ„Ÿæƒ³æ¾æ¹ƒååˆ†ä¹‹ä¸€ã€‚ä¹Ÿçƒ¦æ¼äºé”™è¿‡äº†,è‡³å°‘èƒ½åœ¨é¸Ÿå·¢é™„è¿‘çœ‹ä¸€çœ‹çƒŸèŠ±,æ„Ÿå—é‚£å£®é˜”çš„æœºä¼šã€‚ç»§ç»­å»å‰§é‡Œå­¦è¿‘ä»£å²,å»æ„Ÿå—é‚£ä¸ªæ¿€æ‰¬çš„æ—¶ä»£å§!æ–‡åŒ–è‡ªä¿¡,æˆ‘ä»¬åº”è¯¥,æˆ‘ä»¬å¯ä»¥,æˆ‘ä»¬,éœ€è¦ã€‚", 
	     "å…¸ç±é‡Œçš„ä¸­å›½éƒ½ç»™æˆ‘å»çœ‹!è¿™æ˜¯ä»€ä¹ˆå®è—ç»¼è‰ºå•Šæˆ‘é !",
	     "å…¸ç±é‡Œçš„ä¸­å›½ æ¯ä¸€é›†éƒ½è®©äººæ½¸ç„¶æ³ªä¸‹", 
	     "å…¸ç±é‡Œçš„ä¸­å›½,", 
	     "åœ¨çœ‹CCTV1å…¸ç±é‡Œçš„ä¸­å›½ã€Šæ¥šè¾ã€‹ã€‚æ¥šæ€€ç‹ä¸æ„¿å’Œé½è”æ‰‹æŠ—ç§¦,è€Œæ˜¯è”åˆç§¦å›½,æœ€åå®¢æ­»ç§¦å›½ã€‚ å¯¹å±ˆåŸçš„å»ºè®®:ä½ æ°¸è¿œä¹Ÿå«ä¸é†’ä¸€ä¸ªè£…ç¡çš„äººã€‚ä¸ç”¨ç™½è´¹åŠ›æ°”äº†ã€‚", 
	     "ä¸‰åˆ·ğŸ‰‘ï¸ğŸ‘ ", 
	     "ma", 
	     "ç ",
	     "é©¬",
	     "ğŸ§ğŸ§", 
	     " å¾éœå®¢æ¸¸è®°ä¸€åŒ…çº¸å·¾ğŸ˜­ä¸å¤Ÿç”¨ ", 
	     "å–œæ¬¢è¿™ä¸ª"]

# ä½¿ç”¨ç›¸åŒçš„åˆ†è¯å™¨å¯¹å¥å­è¿›è¡Œå¤„ç†

tokenizer = BertTokenizer.from_pretrained(model_path, cache_dir=None, force_download=False)
tokenized_sentences = tokenizer(sentences, add_special_tokens=True, padding=True, truncation=True, return_tensors='pt', max_length=100)

# å°†å¤„ç†åçš„è¾“å…¥ä¼ é€’ç»™æ¨¡å‹

model.eval()
with torch.no_grad():
    inputs = {key: value.to(device) for key, value in tokenized_sentences.items()}
    outputs = model(**inputs)[0]

# è·å–æ¯ä¸ªæ ·æœ¬çš„é¢„æµ‹ç»“æœï¼ˆé€‰æ‹©æ¦‚ç‡æœ€é«˜çš„æ ‡ç­¾ï¼‰

_, predicted_labels = torch.max(outputs, 1)

# æ‰“å°æ¯ä¸ªæ ·æœ¬çš„é¢„æµ‹ç»“æœ

for i in range(len(sentences)):
    print(f"Sentence: {sentences[i]}, \n Predicted Label: {predicted_labels[i].item()}")
```
é¢„æµ‹ç»“æœ

```python
Sentence: çœ‹å…¸ç±é‡Œçš„ä¸­å›½è¦å¤‡å¥½çº¸æ“¦çœ¼æ³ª, 
Predicted Label: 0
Sentence: è¿™å‡ å¤©,è€³è¾¹çœ¼ä¸­å…¨æ˜¯å»ºå…š100å‘¨å¹´çš„äº‹,ä¹Ÿåˆšå¥½åœ¨çœ‹ã€Šè§‰é†’å¹´ä»£ã€‹,åŠ ä¹‹è¿‘æœŸçœ‹çš„å‰§ä¸æ˜¯å…³äºã€Šç†æƒ³ç…§è€€ä¸­å›½ã€‹,å°±æ˜¯å…³äºä¸­åæ–‡æ˜æºè¿œæµé•¿çš„ã€Šå…¸ç±é‡Œçš„ä¸­å›½ã€‹ã€‚è¶Šå‘è¶Šå‘åœ°åœ¨æ„Ÿæ…¨,è¿‘ä»£å²é‚£ä¸ªæ—¶æ®µçš„äºº,èƒ½åœ¨â€œå±±å¤ªå¤š,åº™å¤ªå°‘,ä¸çŸ¥é“é‚£åº§åº™èƒ½æ•‘ä¸­å›½â€çš„æ—¶å€™,å°±åšå®šç€å»è·µè¡Œ,å»å¥‰çŒ®,å»ç‰ºç‰²,æ˜¯ä½•ç­‰çš„æ¿€æ‰¬ä¸”çƒ­è¡€ã€‚ä¹Ÿè¶Šå‘æ„Ÿæ…¨äºä¸­åæ–‡æ˜ä¸€è·¯èµ°æ¥çš„ä¸æ˜“ä¸å£®é˜”,è½¦åŒè½¨,ä¹¦åŒæ–‡,å„ç§æ€æƒ³ä¸€ä»£ä»£ä¼ æ‰¿ä¸å‘æ‰¬,è¿‘ä»£æ¨å¹¿ç™½è¯æ–‡,æ€æƒ³å¼€åŒ–æ™®åŠ,æœ‰æ—¶å€™ä¼šè®¶å¼‚äº,å…ˆè´¤å‰è¾ˆä»¬æ€ä¹ˆå°±è¿™ä¹ˆå‰å®³,å„ç§é€‰æ‹©éƒ½èƒ½åˆ‡ä¸­æ—¶å¼Šã€‚æ­¤åˆ»åªè§‰å¾—,è‡ªå·±æ–‡ç¬”å¤ªå·®,å†™ä¸å‡ºå¿ƒä¸­çš„æ„Ÿæƒ³æ¾æ¹ƒååˆ†ä¹‹ä¸€ã€‚ä¹Ÿçƒ¦æ¼äºé”™è¿‡äº†,è‡³å°‘èƒ½åœ¨é¸Ÿå·¢é™„è¿‘çœ‹ä¸€çœ‹çƒŸèŠ±,æ„Ÿå—é‚£å£®é˜”çš„æœºä¼šã€‚ç»§ç»­å»å‰§é‡Œå­¦è¿‘ä»£å²,å»æ„Ÿå—é‚£ä¸ªæ¿€æ‰¬çš„æ—¶ä»£å§!æ–‡åŒ–è‡ªä¿¡,æˆ‘ä»¬åº”è¯¥,æˆ‘ä»¬å¯ä»¥,æˆ‘ä»¬,éœ€è¦ã€‚, 
Predicted Label: 2
Sentence: å…¸ç±é‡Œçš„ä¸­å›½éƒ½ç»™æˆ‘å»çœ‹!è¿™æ˜¯ä»€ä¹ˆå®è—ç»¼è‰ºå•Šæˆ‘é !, 
Predicted Label: 0
Sentence: å…¸ç±é‡Œçš„ä¸­å›½ æ¯ä¸€é›†éƒ½è®©äººæ½¸ç„¶æ³ªä¸‹, 
Predicted Label: 0
Sentence: å…¸ç±é‡Œçš„ä¸­å›½,, 
Predicted Label: 0
Sentence: åœ¨çœ‹CCTV1å…¸ç±é‡Œçš„ä¸­å›½ã€Šæ¥šè¾ã€‹ã€‚æ¥šæ€€ç‹ä¸æ„¿å’Œé½è”æ‰‹æŠ—ç§¦,è€Œæ˜¯è”åˆç§¦å›½,æœ€åå®¢æ­»ç§¦å›½ã€‚ å¯¹å±ˆåŸçš„å»ºè®®:ä½ æ°¸è¿œä¹Ÿå«ä¸é†’ä¸€ä¸ªè£…ç¡çš„äººã€‚ä¸ç”¨ç™½è´¹åŠ›æ°”äº†ã€‚, 
Predicted Label: 1
Sentence: ä¸‰åˆ·ğŸ‰‘ï¸ğŸ‘ , 
Predicted Label: 0
Sentence: ma, 
Predicted Label: 0
Sentence: ç , 
Predicted Label: 0
Sentence: é©¬, 
Predicted Label: 0
Sentence: ğŸ§ğŸ§, 
Predicted Label: 0
Sentence:  å¾éœå®¢æ¸¸è®°ä¸€åŒ…çº¸å·¾ğŸ˜­ä¸å¤Ÿç”¨ , 
Predicted Label: 0
Sentence: å–œæ¬¢è¿™ä¸ª, 
Predicted Label: 0
```
 
 
