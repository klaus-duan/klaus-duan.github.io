---
layout:     post
title:      langchainæµç¨‹å›¾
subtitle:   langchainçš„å¤§è‡´æµç¨‹ï¼Œé™„ä»£ç 
date:       2024-06-04
author:     Klaus
header-img: img/post-bg-cook.jpg
catalog: true
tags:
    - å¤§æ¨¡åž‹
    - langchain
    - RAG
---

# å‰è¨€

åœ¨åŠ è½½äº†ä¸€ä¸ªèƒ¡è¯´å…«é“çš„chatglmæ¨¡åž‹åŽï¼Œä¸‹ä¸€æ­¥çš„å·¥ä½œæ˜¯ä¼˜åŒ–å®ƒçš„è¾“å‡ºã€‚è¿™é‡Œé€šè¿‡`RAG`çš„æµç¨‹ï¼Œå°†æœ¬åœ°çš„æ–‡æœ¬åº“ä½œä¸ºllmçš„å‚è€ƒï¼Œä¼˜åŒ–å…¶è¾“å‡ºã€‚

æœ¬é¡¹ç›®ä½¿ç”¨`langchian`æ¡†æž¶ï¼Œå®žçŽ°åŽŸç†å¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œè¿‡ç¨‹åŒ…æ‹¬åŠ è½½æ–‡ä»¶ -> è¯»å–æ–‡æœ¬ -> æ–‡æœ¬åˆ†å‰² -> æ–‡æœ¬å‘é‡åŒ– -> é—®å¥å‘é‡åŒ– -> åœ¨æ–‡æœ¬å‘é‡ä¸­åŒ¹é…å‡ºä¸Žé—®å¥å‘é‡æœ€ç›¸ä¼¼çš„Â `top k`ä¸ª -> åŒ¹é…å‡ºçš„æ–‡æœ¬ä½œä¸ºä¸Šä¸‹æ–‡å’Œé—®é¢˜ä¸€èµ·æ·»åŠ åˆ°Â `prompt`ä¸­ -> æäº¤ç»™Â `LLM`ç”Ÿæˆå›žç­”ã€‚ï¼ˆåˆšåˆšåšåˆ°æç¤ºè¯ï¼Œç›®å‰å¤„åœ¨ä¸€ä¸ªæ¯”è¾ƒå°´å°¬çš„åœ°æ­¥ï¼Œæ˜¾å¡å’ŒðŸªœä¸èƒ½åŒæ—¶å…·å¤‡ã€‚ï¼‰

[å›¾é“¾æŽ¥]

# æ–‡æ¡£ä¸Šä¼ åŠåˆ‡åˆ†

## Unstructured Loader

æˆ‘è¿™é‡Œä¸Šä¼ çš„æ˜¯markdownæ ¼å¼çš„æ–‡ä»¶ï¼Œlangchainæ”¯æŒå¤šç§æ ¼å¼çš„æ–‡ä»¶ä¸Šä¼ ï¼Œä¹Ÿå¯ä»¥ç›´æŽ¥ä¸Šä¼ æ–‡ä»¶å¤¹ã€‚

    from langchain_community.document_loaders import UnstructuredMarkdownLoader

    markdown_path = '/path'

    loader = UnstructuredMarkdownLoader(markdown_path)

    data = loader.load()

    for i in data[0]:   # æŸ¥çœ‹æ ¼å¼
      print(i)

> ('page_content', 'layout:     post\ntitle:      åˆ©ç”¨å…è´¹å¹³å°åŠ è½½chatglm\nsubtitle:   é£žæ¡¨AI studioç®€ä»‹ä»¥åŠchatglmçš„')
> 
>  ('metadata', {'source': '/Users/klaus_d./Desktop/chatglm.md'})
> 
>  ('type', 'Document')

## Text Splitter

æ–‡æ¡£ä¸Šä¼ ä¹‹åŽï¼Œè¿›è¡Œåˆ†å‰²

ä»¥ä¸‹ä¸ºä¸€ä¸ªä¾‹å­ï¼š

	from langchain_text_splitters import MarkdownHeaderTextSplitter 
	# æ­¤å·¥å…·å¯ä»¥æ ¹æ®æ–‡æ¡£çš„æ ‡é¢˜å¯¹æ–‡æ¡£è¿›è¡Œåˆ‡åˆ†
	
	markdown_path = 'md = # Foo\n\n ## Bar\n\nHi this is Jim  \nHi this is Joe\n\n ## Baz\n\n Hi this is Molly' 
	
	headers_to_split_on = [("#", "Header 1"), ("##", "Header 2"), ("###", "Header 3"),]

	markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on)
	
	md_header_splits = markdown_splitter.split_text(markdown_path)
	
	len(md_header_splits) #æ ¹æ®è¦æ±‚è¢«åˆ†æˆäº†3ä»½
	
	for i in md_header_splits[2]:  # æ‰“å°åˆ†å‰²åŽæ–‡æ¡£çš„ç¬¬3å—è¾“å‡º
    	print(i)
	
> ('page_content', 'Hi this is Molly')
> 
> ('metadata', {'Header 2': 'Baz'})
> 
> ('type', 'Document')

åˆ†å‰²æ–‡æ¡£çš„åŒæ—¶ï¼Œè¿˜å¯ä»¥æŒ‡å®šæ–‡æ¡£çš„é•¿åº¦ï¼Œå³`text chunks`ã€‚æ‰€ä»¥`text chunks`å¹¶ä¸éœ€è¦ä¸€ä¸ªä¸“ç”¨çš„åŒ…ã€‚

## Text Chunks

	from langchain_text_splitters import CharacterTextSplitter
	# æ­¤å·¥å…·å¯å°†æ–‡æ¡£æŒ‰å­—ç¬¦åˆ†å‰²
	
	with open('markdown_path') as f:
    	state_of_the_union = f.read()

	text_splitter = CharacterTextSplitter(
    	separator="\n\n",
    	chunk_size=300,     # è®¾ç½®æ¯æ®µåˆ†å‰²çš„é•¿åº¦
    	chunk_overlap=0,    # è®¾ç½®ä¸¤ä¸ªåˆ†å‰²æ®µè½çš„é‡å é•¿åº¦
    	length_function=len,
    	is_separator_regex=False)
    	
	texts = text_splitter.create_documents([state_of_the_union])

	for i in texts[:3]:
    	print(i.page_content)

	len(text_splitter.split_text(state_of_the_union))    # æ–‡æ¡£è¢«åˆ†å‰²æˆ15æ®µ

> Created a chunk of size 345, which is longer than the specified 300
> 
> Created a chunk of size 345, which is longer than the specified 300
> 
> Created a chunk of size 345, which is longer than the specified 300
> 
> Created a chunk of size 345, which is longer than the specified 300
> 
> 227
> 
> 245
> 
> 270

# æ–‡æ¡£ç¼–ç åŠå­˜å‚¨

## Embedding

æ–‡æœ¬è½¬åŒ–ä¸ºå‘é‡ï¼Œè¿™é‡Œç”¨çš„huggingface embeddingçš„`all-mpnet-base-v2`æ¨¡åž‹ï¼Œå³é»˜è®¤æ¨¡åž‹ã€‚

	from langchain_huggingface import HuggingFaceEmbeddings
	
	embeddings_model = HuggingFaceEmbeddings(model_name="sentence-transformers/all-mpnet-base-v2", show_progress=True)
	
	text_to_embedding = [i.page_content for i in texts]
	
	# embed_decuments æŽ¥å— list[string] å­—ç¬¦ä¸²åˆ—è¡¨ï¼Œè¦å¯¹å¤šå¥è¯embeddingæ—¶ä½¿ç”¨ã€‚
	embeddings = embeddings_model.embed_documents(text_to_embedding)   

	# embed_query æŽ¥å—ä¸€ä¸ªå­—ç¬¦ä¸²ï¼Œå¯¹ä¸€å¥è¯ embedding æ—¶ä½¿ç”¨ã€‚
	embedded_query = embeddings_model.embed_query("What was the name mentioned in the conversation?")


æŸ¥çœ‹æ­¤embedding æ¨¡åž‹çš„å‚æ•°ï¼Œ`help(embeddings_model)`

>     HuggingFaceEmbeddings(*, client: Any = None, 
> 							model_name: str = 'sentence-transformers/all-mpnet-base-v2', 
> 							cache_folder: Optional[str] = None, 
> 							model_kwargs: Dict[str, Any] = None, 
> 							encode_kwargs: Dict[str, Any] = None, 
> 							multi_process: bool = False, 
> 							show_progress: bool = False) -> None


## VectorStore

è¿™é‡Œå±•ç¤º`Chroma`å’Œ`FAISS`å‘é‡å­˜å‚¨åº“

Chromaï¼š

	from langchain_chroma import Chroma

	db = Chroma.from_documents(texts, embeddings_model)
	
FAISSï¼š

	from langchain_community.vectorstores import FAISS

	vectorstore = FAISS.from_documents(texts, embeddings_model)

from\_documentsæŽ¥æ”¶ä¸¤ä¸ªå‚æ•°ï¼Œtextsæ˜¯åˆ†å‰²å¥½çš„æ•°æ®ï¼Œæ ¼å¼ä¸ºdocumentsã€‚ä»¥åŠembeddings_model

	help(FAISS.from_documents)

> Help on method from\_documents in module langchain_core.vectorstores:
> 
> from_documents(documents: 'List[Document]', embedding: 'Embeddings', **kwargs: 'Any') -> 'VST' method of abc.ABCMeta instance
> 
>    Return VectorStore initialized from documents and embeddings.


## Vecter Similarity

æ¯”è¾ƒqueryå‘é‡å’Œvector storeä¸­æ–‡æœ¬å‘é‡çš„ç›¸ä¼¼åº¦ï¼Œè¿”å›žæ–‡æœ¬ã€‚

- å½“è¾“å…¥queryä¸ºæ–‡æœ¬æ—¶ï¼š

		query = "chatglmè£…åœ¨å“ªé‡Œ"
	
		docs = db.similarity_search(query)
		
		# é»˜è®¤è¿”å›ž4ä¸ªç»“æžœ
	
	`help(db.similarity_search)`:

	> Args:
	> 
	> query (str): Query text to search for.
	> 
	> k (int): Number of results to return. Defaults to 4.
	> 
	> filter (Optional[Dict[str, str]]): Filter by metadata. Defaults to None.
    
- å½“è¾“å…¥queryä¸ºå‘é‡æ—¶ï¼š

		embedding_vector = embeddings_model.embed_query(query)
		
		docs = db.similarity_search_by_vector(embedding_vector)
	
	`help(db.similarity_search_by_vector)`:
	
	> Args:
	>
	> embedding (List[float]): Embedding to look up documents similar to.
	> 
	> k (int): Number of Documents to return. Defaults to 4.
	> 
	> filter (Optional[Dict[str, str]]): Filter by metadata. Defaults to None.

# Prompt Template & LLM

## Prompt Template

PromptTemplateå¯ä»¥è¢«ç”¨æ¥æŒ‡å¯¼æ¨¡åž‹ç”Ÿæˆç‰¹å®šç±»åž‹çš„å›žç­”æˆ–æ‰§è¡Œç‰¹å®šçš„ä»»åŠ¡ã€‚

PromptTemplateå¯ä»¥å•ç‹¬é…åˆvector storeä½¿ç”¨ï¼Œä¹Ÿå¯ä»¥ä¸ŽLLMç»“åˆä½¿ç”¨ï¼Œä»¥ä¸‹ä¸ºå•ç‹¬é…åˆVector Stroeä½¿ç”¨çš„ç¤ºä¾‹ï¼š

	from langchain_core.prompts import PromptTemplate
	
	example_prompt = PromptTemplate.from_template("Question: {question}\n{answer}")
	examples = [
	    {
        	"question": "Who lived longer, Muhammad Ali or Alan Turing?",
			"answer": """
	Are follow up questions needed here: Yes.
	Follow up: How old was Muhammad Ali when he died?
	Intermediate answer: Muhammad Ali was 74 years old when he died.
	Follow up: How old was Alan Turing when he died?
	Intermediate answer: Alan Turing was 41 years old when he died.
	So the final answer is: Muhammad Ali
	""",
	    }
	]

`print(example_prompt.invoke(examples[0]).to_string())`:

> Question: Who lived longer, Muhammad Ali or Alan Turing?
> 
> Are follow up questions needed here: Yes.
> 
> Follow up: How old was Muhammad Ali when he died?
> 
> Intermediate answer: Muhammad Ali was 74 years old when he died.
> 
> Follow up: How old was Alan Turing when he died?
> 
> Intermediate answer: Alan Turing was 41 years old when he died.
> 
> So the final answer is: Muhammad Ali

## LLM

# è¾“å‡ºanswer